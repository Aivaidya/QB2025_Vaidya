---
title: '3\. Worksheet: Basic R'
author: "Student Name; Z620: Quantitative Biodiversity, Indiana University"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
geometry: margin=2.54cm
---

## OVERVIEW

This worksheet introduces some of the basic features of the R computing environment (http://www.r-project.org).
It is designed to be used along side the **3. RStudio** handout in your binder. 
You will not be able to complete the exercises without the corresponding handout.

## Directions:
1. In the Markdown version of this document in your cloned repo, change "Student Name" on line 3 (above) with your name.
2. Complete as much of the worksheet as possible during class.
3. Use the handout as a guide; it contains a more complete description of data sets along with examples of proper scripting needed to carry out the exercises.
4. Answer questions in the  worksheet.
Space for your answers is provided in this document and is indicated by the ">" character.
If you need a second paragraph be sure to start the first line with ">".
You should notice that the answer is highlighted in green by RStudio (color may vary if you changed the editor theme). 
5. Before you leave the classroom today, you must **push** this file to your GitHub repo, at whatever stage you are. This will enable you to pull your work onto your own computer.
6. When you have completed the worksheet, **Knit** the text and code into a single PDF file by pressing the `Knit` button in the RStudio scripting panel.
This will save the PDF output in your '3.RStudio' folder.
7. After Knitting, please submit the worksheet by making a **push** to your GitHub repo and then create a **pull request** via GitHub.
Your pull request should include this file (**3.RStudio_Worksheet.Rmd**) with all code blocks filled out and questions answered) and the PDF output of `Knitr`   
(**3.RStudio_Worksheet.pdf**).

The completed exercise is due on **Wednesday, January 22^nd^, 2025 before 12:00 PM (noon)**.

## 1) HOW WE WILL BE USING R AND OTHER TOOLS

You are working in an RMarkdown (.Rmd) file.
This allows you to integrate text and R code into a single document.
There are two major features to this document: 1) Markdown formatted text and 2) "chunks" of R code.
Anything in an R code chunk will be interpreted by R when you *Knit* the document.

When you are done, you will *knit* your document together.
However, if there are errors in the R code contained in your Markdown document, you will not be able to knit a PDF file. 
If this happens, you will need to review your code, locate the source of the error(s), and make the appropriate changes.
Even if you are able to knit without issue, you should review the knitted document for correctness and completeness before you submit the Worksheet. Next to the `Knit` button in the RStudio scripting panel there is a spell checker button (`ABC`) button.

## 2) SETTING YOUR WORKING DIRECTORY

In the R code chunk below, please provide the code to: 
1) clear your R environment,
2) print your current working directory, and
3) set your working directory to your '3.RStudio' folder. 

```{r}
getwd()
setwd("/cloud/project/QB2025_Vaidya/Week1-RStudio")
 
```

## 3) USING R AS A CALCULATOR

To follow up on the pre-class exercises, please calculate the following in the R code chunk below. 
Feel free to reference the **1. Introduction to version control and computing tools** handout. 

1) the volume of a cube with length, l, = 5 (volume = l^3 )
2) the area of a circle with radius, r, = 2 (area = pi * r^2). 
3) the length of the opposite side of a right-triangle given that the angle, theta, = pi/4. (radians, a.k.a. 45Â°) and with hypotenuse length sqrt(2) (remember: sin(theta) = opposite/hypotenuse).
4) the log (base e) of your favorite number.

```{r}
l<-5
volume<-l^3
volume 
 125
 r<-2
area<-pi*r^2
area
12.56637
sin(theta)
0.8509035
 opposite<-hypotenuse*sin(theta)
opposite
 1.701807
log10(1000)
 3
```

## 4) WORKING WITH VECTORS

To follow up on the pre-class exercises, please perform the requested operations in the R-code chunks below.

### Basic Features Of Vectors

In the R-code chunk below, do the following: 
1) Create a vector `x` consisting of any five numbers.
2) Create a new vector `w` by multiplying `x` by 14 (i.e., "scalar").
3) Add `x` and `w` and divide by 15.

```{r}
x <- c(2,5,7,8,11)
w <- x*14
w

(x+w)/15



```

Now, do the following: 
1) Create another vector (`k`) that is the same length as `w`.
2) Multiply `k` by `x`.
3) Use the combine function to create one more vector, `d` that consists of any three elements from `w` and any four elements of `k`.

```{r}
k<-c(21,24,15,78,66)
 k*x
42 120 105 624 726
rm(w)
 rm(k)
 w<-c(28,70,112)
 k<-c(21,24,78,66) 
d<-c(w)[c(k)]
 d
 NA NA NA NA

```

### Summary Statistics of Vectors

In the R-code chunk below, calculate the **summary statistics** (i.e., maximum, minimum, sum, mean, median, variance, standard deviation, and standard error of the mean) for the vector (`v`) provided.

```{r}
v <- c(16.4, 16.0, 10.1, 16.8, 20.5, NA, 20.2, 13.1, 24.8, 20.2, 25.0, 20.5, 30.5, 31.4, 27.1)
 maximum <- max(v, na.rm = TRUE) 
 maximum
 31.4; 
 minimum <- min(v, na.rm = TRUE)
 minimum
 10.1
 sum(v, na.rm = TRUE)
 292.6
 mean(v, na.rm = TRUE)
 20.9
 median(v, na.rm = TRUE)
 20.35
variance <- var(v, na.rm = TRUE)
 variance
39.44
 sd(v, na.rm = TRUE)
 6.280127
sem <- function (x){sd(x)/sqrt(length(x))}
sem <- function(x){sd(na.omit(x))/sqrt(length(na.omit(x)))}
sem(v)
1.678435





```

## 5) WORKING WITH MATRICES

In the R-code chunk below, do the following:
Using a mixture of Approach 1 and 2 from the **3. RStudio** handout, create a matrix with two columns and five rows.
Both columns should consist of random numbers.
Make the mean of the first column equal to 8 with a standard deviation of 2 and the mean of the second column equal to 25 with a standard deviation of 10.

```{r}

column1 <- rnorm(5, mean = 8, sd = 2)
column2 <- rnorm(5, mean = 25, sd = 10)
my_matrix <- matrix(c(column1, column2), nrow =5, ncol = 2)
my_matrix
          [,1]     [,2]
[1,]  8.113370 37.82748
[2,]  7.620750 23.98472
[3,]  7.433316 20.81717
[4,]  7.573189 28.83781
[5,] 10.713674 22.25251


## This isn't code and it can be inside a code chunk as long as there is a # in front


```

***Question 1***: What does the `rnorm` function do? 
What do the arguments in this function specify? 
Remember to use `help()` or type `?rnorm`.

> Answer 1:
?rnorm
rnorm function is used to create random numbers from a normal distribution for a specified mean and standard deviation.Arguments in this function like n specify the spread of random numbers(i.e number of observations), mean specifies the mean of the data(vector of means), sd i.e standard deviation specifies the standard deviation in adherence to which the random numbers generated. It also specifies vector of quintiles by typing x, q and vector of probabilities by typing p. log, log.p suggests that propbabilities p are written as log(p). Otherwise it also gives a range where lower.tail when true is written as P[X<_x] else the opposite holds true. Thus this allows rnorm allows one to control sd and mean while predicting the rest.

In the R code chunk below, do the following: 
1) Load `matrix.txt` from the **3.RStudio** data folder as matrix `m`.
2) Transpose this matrix.
3) Determine the dimensions of the transposed matrix.

>setwd("/cloud/project/QB2025_Vaidya/Week1-RStudio/data")

```{r}
data <- read.table("/cloud/project/QB2025_Vaidya/Week1-RStudio/data/matrix.txt")
data <- read.table("data/matrix.txt")
m <- as.matrix(read.table("/cloud/project/QB2025_Vaidya/Week1-RStudio/data/matrix.txt", sep = "\t", header = FALSE))
 n <- t(m)
 dim(n)

```


***Question 2***: What are the dimensions of the matrix you just transposed?

> Answer 2:
 5 10

###Indexing a Matrix

In the R code chunk below, do the following:
1) Index matrix `m` by selecting all but the third column.
2) Remove the last row of matrix `m`.

```{r}
 n <- m[ , -3]
n <- m[1:4, ]
n<- c( m[1:4, ], m[ , -3])

```

## 6) BASIC DATA VISUALIZATION AND STATISTICAL ANALYSIS
### Load Zooplankton Data Set

In the R code chunk below, do the following:
1) Load the zooplankton data set from the **3.RStudio** data folder.
2) Display the structure of this data set.

```{r}
> meso <- read.table("/cloud/project/QB2025_Vaidya/Week1-RStudio/data/zoop_nuts.txt" , sep = "\t" , header = TRUE)
> str(meso)

'data.frame':	24 obs. of  8 variables:
 $ TANK: int  34 14 23 16 21 5 25 27 30 28 ...
 $ NUTS: chr  "L" "L" "L" "L" ...
 $ TP  : num  20.3 25.6 14.2 39.1 20.1 ...
 $ TN  : num  720 750 610 761 570 ...
 $ SRP : num  4.02 1.56 4.97 2.89 5.11 4.68 5 0.1 7.9 3.92 ...
 $ TIN : num  131.6 141.1 107.7 71.3 80.4 ...
 $ CHLA: num  1.52 4 0.61 0.53 1.44 1.19 0.37 0.72 6.93 0.94 ...
 $ ZP  : num  1.781 0.409 1.201 3.36 0.733 ...



```

### Correlation

In the R-code chunk below, do the following:
1) Create a matrix with the numerical data in the `meso` dataframe.
2) Visualize the pairwise **bi-plots** of the six numerical variables.
3) Conduct a simple **Pearson's correlation** analysis.

```{r}
meso.num <- meso[  ,3:8]
meso.num
      TP     TN   SRP     TIN  CHLA     ZP
1   20.31  720.1  4.02  131.62  1.52 1.7808
2   25.55  750.5  1.56  141.10  4.00 0.4090
3   14.22  610.1  4.97  107.70  0.61 1.2014
4   39.11  760.9  2.89   71.28  0.53 3.3598
5   20.09  570.4  5.11   80.40  1.44 0.7332
6   15.75  680.5  4.68  135.77  1.19 0.9773
7   19.55  665.5  5.00   79.40  0.37 1.0999
8   16.19  660.8  0.10  100.91  0.72 2.2714
9   29.46 1770.4  7.90 1329.26  6.93 3.1633
10  37.88 2590.3  3.92 1163.64  0.94 1.8747
11  30.26 2110.9  4.45 1850.18  1.36 4.3802
12  36.94 2060.9  5.14  249.93 38.38 2.4051
13  34.73 1370.1  4.69  420.01 15.99 1.7079
14  26.00 2110.3  5.35 1466.70  0.95 4.0999
15  28.50 1760.4  7.15 1351.83  1.36 5.4430
16  35.33 1360.8  5.96 1036.27  2.13 4.2677
17  41.56 4130.1 20.34 3421.43  1.44 8.2084
18  53.50 4530.4 33.57 4042.10  0.93 4.2273
19  99.07 4410.9 11.57 3307.05  0.61 6.2381
20 128.04 4750.4 26.27 3686.17  1.27 8.5713
21  33.47 3410.4  9.32 2791.52  1.11 1.4240
22  52.41 3710.3  3.23 2890.73 17.59 2.9714
23  42.21 3690.4 12.71 3041.75  1.08 8.1509
24  77.65 4380.6 21.86 3041.75  1.08 8.3868
cor1 <- cor(meso.num)
cor1
              TP           TN        SRP        TIN         CHLA         ZP
TP    1.00000000  0.786510407  0.6540957  0.7171143 -0.016659593  0.6974765
TN    0.78651041  1.000000000  0.7841904  0.9689999 -0.004470263  0.7562474
SRP   0.65409569  0.784190400  1.0000000  0.8009033 -0.189148017  0.6762947
TIN   0.71711434  0.968999866  0.8009033  1.0000000 -0.156881463  0.7605629
CHLA -0.01665959 -0.004470263 -0.1891480 -0.1568815  1.000000000 -0.1825999
ZP    0.69747649  0.756247384  0.6762947  0.7605629 -0.182599904  1.0000000
```


***Question 3***: Describe some of the general features based on the visualization and correlation analysis above?

> Answer 3:Except for CHLA, All other groups have a positive pearson's correlation with each other. Of the groups, correlation of TN with TIN has a strong positive correlation of 0.969, suggesting the presence of TIN positively reinforces TN. CHLA is negatively correlated to all the other variables within the dataset.


In the R code chunk below, do the following:
1) Redo the correlation analysis using the `corr.test()` function in the `psych` package with the following options: method = "pearson", adjust = "BH".
2) Now, redo this correlation analysis using a non-parametric method.
3) Use the print command from the handout to see the results of each correlation analysis.

```{r}
1. cor2 <- corr.test(meso.num, method = "pearson", adjust = "BH")
> print(cor2, digits = 3)
Call:corr.test(x = meso.num, method = "pearson", adjust = "BH")
Correlation matrix 
         TP     TN    SRP    TIN   CHLA     ZP
TP    1.000  0.787  0.654  0.717 -0.017  0.697
TN    0.787  1.000  0.784  0.969 -0.004  0.756
SRP   0.654  0.784  1.000  0.801 -0.189  0.676
TIN   0.717  0.969  0.801  1.000 -0.157  0.761
CHLA -0.017 -0.004 -0.189 -0.157  1.000 -0.183
ZP    0.697  0.756  0.676  0.761 -0.183  1.000
Sample Size 
[1] 24
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
        TP    TN   SRP   TIN  CHLA    ZP
TP   0.000 0.000 0.001 0.000 0.983 0.000
TN   0.000 0.000 0.000 0.000 0.983 0.000
SRP  0.001 0.000 0.000 0.000 0.491 0.000
TIN  0.000 0.000 0.000 0.000 0.536 0.000
CHLA 0.938 0.983 0.376 0.464 0.000 0.491
ZP   0.000 0.000 0.000 0.000 0.393 0.000

2.  cor2 <- corr.test(meso.num, method = "spearman", adjust = "BH")
> print(cor2, digits = 3)
Call:corr.test(x = meso.num, method = "spearman", adjust = "BH")
Correlation matrix 
        TP    TN    SRP   TIN   CHLA     ZP
TP   1.000 0.895  0.539 0.761  0.040  0.741
TN   0.895 1.000  0.647 0.942  0.021  0.748
SRP  0.539 0.647  1.000 0.726 -0.064  0.627
TIN  0.761 0.942  0.726 1.000  0.088  0.738
CHLA 0.040 0.021 -0.064 0.088  1.000 -0.072
ZP   0.741 0.748  0.627 0.738 -0.072  1.000
Sample Size 
[1] 24
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
        TP    TN   SRP   TIN  CHLA    ZP
TP   0.000 0.000 0.010 0.000 0.914 0.000
TN   0.000 0.000 0.001 0.000 0.923 0.000
SRP  0.007 0.001 0.000 0.000 0.884 0.002
TIN  0.000 0.000 0.000 0.000 0.884 0.000
CHLA 0.853 0.923 0.767 0.683 0.000 0.884
ZP   0.000 0.000 0.001 0.000 0.737 0.000

```

***Question 4***: 
Describe what you learned from `corr.test`. 
Specifically, are the results sensitive to whether you use parametric (i.e., Pearson's) or non-parametric methods?
When should one use non-parametric methods instead of parametric methods?
With the Pearson's method, is there evidence for false discovery rate due to multiple comparisons? 
Why is false discovery rate important?

> Answer 4:1. Yes we can certainly see differences. Since spearman is a non-prarametric test, it is far more stringent than the parametric test, thus the pearson test shows a stronger correlation with most variables than the spearman, as the spearman also considers any outliers unlike the pearson test that is less sensitive. p values between them are also different when comparing varibles like TPN and SRP has a pvlue of 0.01 for spearman test while 0.001 for pearson's test. 
2. If the data is normally distributed then pearson's test (parametric) would be more apt, while when the data are not normally distributed with the presence of significant outliers, then one will consider non-parametric test like spearman. False discovery rate strikes the right balance between the very stringent tests like Bonferroni that sometimes prevents true detectable difference and also prevents the other extreme Type I errors which are created due to false detection of a significant difference as a result of many variables being tested at the same time. False discovery rate limits Type I error while preventing a true significant difference from being unnoticed. 

### Linear Regression

In the R code chunk below, do the following:
1) Conduct a linear regression analysis to test the relationship between total nitrogen (TN) and zooplankton biomass (ZP).
2) Examine the output of the regression analysis.
3) Produce a plot of this regression analysis including the following: categorically labeled points, the predicted regression line with 95% confidence intervals, and the appropriate axis labels.

```{r}
fitreg <- lm(ZP ~ TN, data = meso)
> summary(fitreg)

Call:
lm(formula = ZP ~ TN, data = meso)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7690 -0.8491 -0.0709  1.6238  2.5888 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)    
(Intercept) 0.6977712  0.6496312   1.074    0.294    
TN          0.0013181  0.0002431   5.421 1.91e-05 ***
---
Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

Residual standard error: 1.75 on 22 degrees of freedom
Multiple R-squared:  0.5719,	Adjusted R-squared:  0.5525 
F-statistic: 29.39 on 1 and 22 DF,  p-value: 1.911e-05

> plot(meso$TN, meso$ZP, ylim = c(0, 10), xlim = c(500,5000), xlab = expression(paste("Total Nitrogen (" , mu,"g/L")), ylab = "Zooplankton Biomass (mg/L)", las =1)
> text(meso$TN, meso$ZP, meso$NUTS, pos = 3, cex = 0.8)
> newTN <- seq(min(meso$TN), max(meso$TN), 10)
> regline <- predict(fitreg, newdata = data.frame(TN = newTN))
> lines(newTN, regline)
> conf95 <- predict(fitreg, newdata = data.frame(TN = newTN), interval = c("confidence"), level = 0.95, type = "response")
> matlines(newTN, conf95[, c("lwr", "upr")], type ="l", lty = 2, lwd = 1, col = "black")





```




***Question 5***: Interpret the results from the regression model

> Answer 5:Although the regression line roughly follows a straight line, the points are scattered along the fitted regression line, making it a weak relationship.Since the direction of the slope is upwards, i.e an increase in x increases y, it is a weak positive relationship. Also since some points lie outside the confidence intervals suggests greater uncertainty in predictions. Also since the residuals and the point closely follow the reference line and fitted line respectively, the data does follow a normal distribution with a fe outliers. Many of these outliers could indicate that they disproportionately influence the model.


```{r}

```

### Analysis of Variance (ANOVA)

Using the R code chunk below, do the following:
1) Order the nutrient treatments from low to high (see handout).
2) Produce a barplot to visualize zooplankton biomass in each nutrient treatment.
3) Include error bars (+/- 1 sem) on your plot and label the axes appropriately.
4) Use a one-way analysis of variance (ANOVA) to test the null hypothesis that zooplankton biomass is affected by the nutrient treatment.


```{r}
NUTS <- factor(meso$NUTS, levels = c('L', 'M', 'H'))
zp.means <- tapply(meso$ZP, NUTS, mean)
> sem <- function(x){sd(na.omit(x))/sqrt(length(na.omit(x)))}
> zp.sem <-tapply(meso$ZP, NUTS, sem)
> bp<-barplot(zp.means, ylim = c(0, round(max(meso$ZP), digits = 0)), pch =15, cex = 1.25, las =1, cex.lab = 1.4, cex.axis = 1.25, xlab = "nutrient supply", ylab = "zooplankton biomass (mg/L)", names.arg = c("low", "medium", "high"))
> arrows(x0 =bp, y0 = zp.means, y1 = zp.means - zp.sem, angle = 90, length =0.1, lwd = 1)
> arrows(x0 =bp, y0 = zp.means, y1 = zp.means+ zp.sem, angle = 90, length = 0.1, lwd = 1)


 fitanova <- aov(ZP ~ NUTS, data = meso)
> summary(fitanova)
            Df Sum Sq Mean Sq F value   Pr(>F)    
NUTS         2  83.15   41.58   11.77 0.000372 ***
Residuals   21  74.16    3.53                     
---
Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1
TukeyHSD(fitanova)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = ZP ~ NUTS, data = meso)

$NUTS
         diff        lwr        upr     p adj
L-H -4.543175 -6.9115094 -2.1748406 0.0002512
M-H -2.604550 -4.9728844 -0.2362156 0.0294932
M-L  1.938625 -0.4297094  4.3069594 0.1220246
Tukey confirmed significant difference between low-medium and medium-high with p value sitting at 0.0002 and 0.029 respectively.


```

## SYNTHESIS: SITE-BY-SPECIES MATRIX

In the R code chunk below, load the zoops.txt data set in your **3.RStudio** data folder.
Create a site-by-species matrix (or dataframe) that does *not* include TANK or NUTS.
The remaining columns of data refer to the biomass (Âµg/L) of different zooplankton taxa: 
  
  + CAL = calanoid copepods
  
  + DIAP = *Diaphanasoma* sp. 
  
  + CYL = cyclopoid copepods
  
  + BOSM = *Bosmina* sp.
  
  + SIMO = *Simocephallus* sp.
  
  + CERI = *Ceriodaphnia* sp.
  
  + NAUP = naupuli (immature copepod)
  
  + DLUM = *Daphnia lumholtzi*
  
  + CHYD = *Chydorus* sp. 

***Question 6***: With the visualization and statistical tools that we learned about in the **3. RStudio** handout, use the site-by-species matrix to assess whether and how different zooplankton taxa were responsible for the total biomass (ZP) response to nutrient enrichment. 
Describe what you learned below in the "Answer" section and include appropriate code in the R chunk.

```{r}
zoops <- read.table("/cloud/project/QB2025_Vaidya/Week1-RStudio/data/zoops.txt", header = TRUE, sep = "\t")
str(zoops)
'data.frame':	24 obs. of  11 variables:
 $ TANK: int  5 14 16 21 23 25 27 34 12 15 ...
 $ NUTS: chr  "L" "L" "L" "L" ...
 $ CAL : num  70.5 27.1 5.3 79.2 31.4 22.7 0 35.7 74.8 5.3 ...
 $ DIAP: num  0 19.2 8.8 17.9 0 ...
 $ CYCL: num  66.1 129.6 12.7 141.3 11 ...
 $ BOSM: num  2.2 0 0 3.4 0 0 0 0 0 0 ...
 $ SIMO: num  417.8 0 73.1 0 482 ...
 $ CERI: num  159.8 79.4 107.5 199 101.9 ...
 $ NAUP: num  0 0 1.2 0 0 1.2 1.6 3.1 0 1.4 ...
 $ DLUM: num  0 0 0 0 0 6.6 0 0 0 0 ...
 $ CHYD: num  267 159 3158 298 580 ...
> zoops_species <- zoops[, !(colnames(zoops) %in% c("TANK", "NUTS"))]
> zoops_species
     CAL  DIAP  CYCL BOSM   SIMO  CERI NAUP DLUM   CHYD
1   70.5   0.0  66.1  2.2  417.8 159.8  0.0  0.0  266.9
2   27.1  19.2 129.6  0.0    0.0  79.4  0.0  0.0  158.7
3    5.3   8.8  12.7  0.0   73.1 107.5  1.2  0.0 3158.2
4   79.2  17.9 141.3  3.4    0.0 199.0  0.0  0.0  298.5
5   31.4   0.0  11.0  0.0  482.0 101.9  0.0  0.0  580.2
6   22.7 285.1 153.0  0.0  241.5 135.5  1.2  6.6  262.4
7    0.0   2.3  11.0  0.0   73.1 185.0  1.6  0.0 2004.4
8   35.7  65.9 102.9  0.0    0.0 318.5  3.1  0.0 1260.7
9   74.8 178.7 266.5  0.0    0.0   1.9  0.0  0.0 1190.9
10   5.3   4.9  87.8  0.0 1099.2 136.4  1.4  0.0 2939.6
11  18.4   2.3  29.4  0.0  393.8 147.6  1.2  0.0 4857.3
12  14.0   2.3  37.7  0.0 1251.5  74.8  0.0  0.0 2725.5
13  14.0   2.3 132.9  0.0  818.6  98.1  1.2  0.0  814.5
14  48.8   2.3 107.9  2.2    9.0 132.7  0.0  0.0 2867.5
15   0.0   0.0  17.7  0.0  145.3  19.7  0.0  0.0 4201.6
16 292.0 269.5 373.4 10.7    0.0   8.5  1.2  0.0 1456.8
17   9.7   0.0  41.1  0.0 2397.8   9.4  0.0  0.0 5697.9
18   0.0   2.3   0.0  0.0  225.5  24.3  0.0  0.0 8323.2
19   5.3   0.0  86.2  0.0  465.9 527.7  1.2  0.0 3146.9
20  14.0   7.5  69.5  0.0  594.2  78.5  0.0  0.0 7629.2
21   0.0  24.4 101.2  0.0  313.6 176.6  0.0  0.0 7597.6
22   0.0   7.5 253.2  8.3    0.0 112.1  1.6  0.0 2594.8
23   5.3   2.3  96.2  0.0  786.6  76.6  0.0  0.0  463.0
24   0.0   2.3  66.1  0.0  826.7  85.1  0.0  0.0 5263.0
> cor1<- cor(zoops_species)
> require("psych")
> cor2 <- corr.test(zoops_species, method ="pearson", adjust = "BH")
> print(cors, digits = 3)
Error: object 'cors' not found
> print(cor2, digits = 3)
Call:corr.test(x = zoops_species, method = "pearson", adjust = "BH")
Correlation matrix 
        CAL   DIAP   CYCL   BOSM   SIMO   CERI   NAUP   DLUM   CHYD
CAL   1.000  0.643  0.712  0.728 -0.271 -0.191  0.058 -0.034 -0.322
DIAP  0.643  1.000  0.694  0.381 -0.287 -0.172  0.217  0.637 -0.314
CYCL  0.712  0.694  1.000  0.747 -0.325 -0.132  0.186  0.125 -0.369
BOSM  0.728  0.381  0.747  1.000 -0.308 -0.141  0.179 -0.086 -0.206
SIMO -0.271 -0.287 -0.325 -0.308  1.000 -0.183 -0.237 -0.077  0.262
CERI -0.191 -0.172 -0.132 -0.141 -0.183  1.000  0.475  0.020 -0.135
NAUP  0.058  0.217  0.186  0.179 -0.237  0.475  1.000  0.148 -0.238
DLUM -0.034  0.637  0.125 -0.086 -0.077  0.020  0.148  1.000 -0.224
CHYD -0.322 -0.314 -0.369 -0.206  0.262 -0.135 -0.238 -0.224  1.000
Sample Size 
[1] 24
Probability values (Entries above the diagonal are adjusted for multiple tests.) 
       CAL  DIAP  CYCL  BOSM  SIMO  CERI  NAUP  DLUM  CHYD
CAL  0.000 0.005 0.001 0.001 0.479 0.580 0.835 0.901 0.395
DIAP 0.001 0.000 0.002 0.298 0.449 0.582 0.556 0.005 0.395
CYCL 0.000 0.000 0.000 0.001 0.395 0.646 0.580 0.650 0.306
BOSM 0.000 0.066 0.000 0.000 0.395 0.646 0.580 0.774 0.572
SIMO 0.199 0.175 0.122 0.143 0.000 0.580 0.531 0.788 0.485
CERI 0.371 0.421 0.538 0.510 0.393 0.000 0.098 0.925 0.646
NAUP 0.789 0.309 0.385 0.403 0.265 0.019 0.000 0.646 0.531
DLUM 0.876 0.001 0.560 0.688 0.722 0.925 0.491 0.000 0.554
CHYD 0.125 0.136 0.076 0.334 0.216 0.528 0.263 0.293 0.000
> fitreg<- lm(CHYD ~ DIAP, data = zoops)
> summary(fitreg)

Call:
lm(formula = CHYD ~ DIAP, data = zoops)

Residuals:
    Min      1Q  Median      3Q     Max 
-3000.2 -1641.5  -330.6  1103.9  5078.0 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 3267.088    549.920   5.941 5.59e-06 ***
DIAP          -9.529      6.153  -1.549    0.136    
---
Signif. codes:  0 â***â 0.001 â**â 0.01 â*â 0.05 â.â 0.1 â â 1

Residual standard error: 2441 on 22 degrees of freedom
Multiple R-squared:  0.09832,	Adjusted R-squared:  0.05733 
F-statistic: 2.399 on 1 and 22 DF,  p-value: 0.1357

Since the p-value is more than 0.05 as per linear regression, there is no significant difference between groups and majority of them are positively correlated.
```

## SUBMITTING YOUR WORKSHEET
Use Knitr to create a PDF of your completed **3.RStudio_Worksheet.Rmd** document, push the repo to GitHub, and create a pull request.
Please make sure your updated repo include both the PDF and RMarkdown files.

This assignment is due on **Wednesday, January 22^nd^, 2025 at 12:00 PM (noon)**.

